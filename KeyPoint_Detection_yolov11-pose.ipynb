{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4295ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading YOLOv11-pose model...\n",
      "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n-pose.pt to 'yolo11n-pose.pt': 100% ━━━━━━━━━━━━ 6.0MB 13.0MB/s 0.5s.4s<0.0s1s\n",
      "YOLO model loaded successfully on device: cuda\n",
      "Processing video: S_N_332_resized.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Visualizing with YOLO: 100%|██████████| 60/60 [00:02<00:00, 27.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Visualization complete! Video saved to: keypoint_visualization_yolo.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision \n",
    "from ultralytics import YOLO\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "DATASET_DIR = \"Dataset\"  # Path to the dataset directory\n",
    "OUTPUT_VIDEO_PATH = \"keypoint_visualization_yolo.mp4\"\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# --- Load YOLOv11-Pose Model ---\n",
    "try:\n",
    "    print(\"Loading YOLOv11-pose model...\")\n",
    "    # Using 'yolov8n-pose.pt' as it's a standard small model. \n",
    "    # Replace with 'yolo11n-pose.pt' if you have that specific file.\n",
    "    model = YOLO(\"yolo11n-pose.pt\") \n",
    "    model.to(DEVICE)\n",
    "    print(f\"YOLO model loaded successfully on device: {DEVICE}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading YOLO model: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- Keypoint Drawing Utilities (No changes needed here) ---\n",
    "KEYPOINT_DICT = {\n",
    "    'nose': 0, 'left_eye': 1, 'right_eye': 2, 'left_ear': 3, 'right_ear': 4,\n",
    "    'left_shoulder': 5, 'right_shoulder': 6, 'left_elbow': 7, 'right_elbow': 8,\n",
    "    'left_wrist': 9, 'right_wrist': 10, 'left_hip': 11, 'right_hip': 12,\n",
    "    'left_knee': 13, 'right_knee': 14, 'left_ankle': 15, 'right_ankle': 16\n",
    "}\n",
    "SKELETON_EDGES = [\n",
    "    (KEYPOINT_DICT['left_shoulder'], KEYPOINT_DICT['right_shoulder']), (KEYPOINT_DICT['left_hip'], KEYPOINT_DICT['right_hip']),\n",
    "    (KEYPOINT_DICT['left_shoulder'], KEYPOINT_DICT['left_hip']), (KEYPOINT_DICT['right_shoulder'], KEYPOINT_DICT['right_hip']),\n",
    "    (KEYPOINT_DICT['left_shoulder'], KEYPOINT_DICT['left_elbow']), (KEYPOINT_DICT['left_elbow'], KEYPOINT_DICT['left_wrist']),\n",
    "    (KEYPOINT_DICT['right_shoulder'], KEYPOINT_DICT['right_elbow']), (KEYPOINT_DICT['right_elbow'], KEYPOINT_DICT['right_wrist']),\n",
    "    (KEYPOINT_DICT['left_hip'], KEYPOINT_DICT['left_knee']), (KEYPOINT_DICT['left_knee'], KEYPOINT_DICT['left_ankle']),\n",
    "    (KEYPOINT_DICT['right_hip'], KEYPOINT_DICT['right_knee']), (KEYPOINT_DICT['right_knee'], KEYPOINT_DICT['right_ankle']),\n",
    "    (KEYPOINT_DICT['nose'], KEYPOINT_DICT['left_eye']), (KEYPOINT_DICT['nose'], KEYPOINT_DICT['right_eye']),\n",
    "    (KEYPOINT_DICT['left_eye'], KEYPOINT_DICT['left_ear']), (KEYPOINT_DICT['right_eye'], KEYPOINT_DICT['right_ear']),\n",
    "]\n",
    "\n",
    "def draw_keypoints(frame, keypoints, confidence_threshold=0.2):\n",
    "    h, w, _ = frame.shape\n",
    "    for kp in keypoints:\n",
    "        y, x, conf = kp\n",
    "        if conf > confidence_threshold:\n",
    "            # Note: keypoints are already normalized [y, x], so we multiply by h, w\n",
    "            cv2.circle(frame, (int(x * w), int(y * h)), 4, (0, 255, 0), -1)\n",
    "\n",
    "def draw_skeleton(frame, keypoints, confidence_threshold=0.2):\n",
    "    h, w, _ = frame.shape\n",
    "    for start_idx, end_idx in SKELETON_EDGES:\n",
    "        start_kp, end_kp = keypoints[start_idx], keypoints[end_idx]\n",
    "        if start_kp[2] > confidence_threshold and end_kp[2] > confidence_threshold:\n",
    "            start_point = (int(start_kp[1] * w), int(start_kp[0] * h))\n",
    "            end_point = (int(end_kp[1] * w), int(end_kp[0] * h))\n",
    "            cv2.line(frame, start_point, end_point, (255, 0, 0), 2)\n",
    "\n",
    "def get_keypoints_from_result(result, frame_h, frame_w):\n",
    "    \"\"\"\n",
    "    Extracts, normalizes, and formats keypoints from a YOLO result.\n",
    "    Handles multi-person detection by picking the most confident person.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: A NumPy array of shape (17, 3) with normalized [y, x, confidence].\n",
    "    \"\"\"\n",
    "    if result.keypoints is None or len(result.keypoints.data) == 0:\n",
    "        return np.zeros((17, 3), dtype=np.float32)\n",
    "\n",
    "    kpts_tensor = result.keypoints.data\n",
    "    \n",
    "    # --- Handle multiple people: select the one with the highest avg confidence ---\n",
    "    if kpts_tensor.shape[0] > 1:\n",
    "        confidences = kpts_tensor[:, :, 2].mean(dim=1)\n",
    "        best_person_idx = confidences.argmax()\n",
    "        person_kpts = kpts_tensor[best_person_idx]\n",
    "    else:\n",
    "        person_kpts = kpts_tensor[0]\n",
    "\n",
    "    # Convert to numpy and normalize\n",
    "    person_kpts_np = person_kpts.cpu().numpy()\n",
    "    \n",
    "    # Format to [y, x, confidence] and normalize\n",
    "    formatted_kps = np.zeros((17, 3), dtype=np.float32)\n",
    "    for i in range(17):\n",
    "        x, y, conf = person_kpts_np[i]\n",
    "        formatted_kps[i] = [y / frame_h, x / frame_w, conf]\n",
    "        \n",
    "    return formatted_kps\n",
    "\n",
    "def process_and_visualize_video(video_path: str, output_path: str, yolo_model):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open video {video_path}\")\n",
    "        return\n",
    "\n",
    "    frame_width, frame_height = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps, num_frames = int(cap.get(cv2.CAP_PROP_FPS)), int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "    print(f\"Processing video: {os.path.basename(video_path)}\")\n",
    "    for _ in tqdm(range(num_frames), desc=\"Visualizing with YOLO\"):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: break\n",
    "\n",
    "        # --- Run YOLO Inference ---\n",
    "        # The model expects BGR frames, which cv2.read() provides\n",
    "        results = yolo_model(frame, verbose=False)\n",
    "        \n",
    "        # --- Extract, normalize, and format keypoints ---\n",
    "        # We process the first (and likely only) result\n",
    "        keypoints = get_keypoints_from_result(results[0], frame_height, frame_width)\n",
    "\n",
    "        # --- Drawing on the frame (this happens on CPU) ---\n",
    "        vis_frame = frame.copy()\n",
    "        draw_keypoints(vis_frame, keypoints)\n",
    "        draw_skeleton(vis_frame, keypoints)\n",
    "        out.write(vis_frame)\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    print(f\"\\nVisualization complete! Video saved to: {output_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    all_videos = []\n",
    "    for label_name in [\"Fall\", \"No_Fall\"]:\n",
    "        video_folder = os.path.join(DATASET_DIR, label_name, \"Raw_Video\")\n",
    "        if os.path.exists(video_folder):\n",
    "            all_videos.extend([os.path.join(video_folder, f) for f in os.listdir(video_folder)])\n",
    "\n",
    "    if not all_videos:\n",
    "        print(\"Error: No videos found. Check DATASET_DIR path.\")\n",
    "    else:\n",
    "        random_video_path = random.choice(all_videos)\n",
    "        process_and_visualize_video(random_video_path, OUTPUT_VIDEO_PATH, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51826e6c",
   "metadata": {},
   "source": [
    "## run on full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63b72672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading YOLO-Pose Model: yolo11n-pose.pt ---\n",
      "Model loaded successfully on device: cuda\n",
      "Found 6988 videos to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing W090.mp4: 100%|██████████| 6988/6988 [2:42:30<00:00,  1.40s/it]                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Pre-processing complete! ---\n",
      "All keypoint data has been saved to: processed_keypoints_yolo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Configuration ---\n",
    "# The root directory of your raw video dataset\n",
    "DATASET_DIR = \"Dataset\" \n",
    "# The directory where the processed .npy files will be saved\n",
    "OUTPUT_DIR = \"processed_keypoints_yolo\" \n",
    "# Use 'yolov8n-pose.pt' for the smallest model, or specify your 'yolo11n-pose.pt'\n",
    "MODEL_NAME = \"yolo11n-pose.pt\"\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# --- 1. Model Loading ---\n",
    "def load_yolo_model():\n",
    "    \"\"\"Loads the YOLO-Pose model.\"\"\"\n",
    "    print(f\"--- Loading YOLO-Pose Model: {MODEL_NAME} ---\")\n",
    "    try:\n",
    "        model = YOLO(MODEL_NAME)\n",
    "        model.to(DEVICE)\n",
    "        print(f\"Model loaded successfully on device: {DEVICE}\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"FATAL: Error loading YOLO model: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- 2. Main Processing Logic ---\n",
    "def get_main_person_keypoints(result, frame_h, frame_w):\n",
    "    \"\"\"\n",
    "    Extracts, normalizes, and formats keypoints from a YOLO result.\n",
    "    \n",
    "    Handles multi-person detection by picking the person with the largest bounding box area.\n",
    "    This version is robust against frames where boxes are detected but keypoints are not.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: A NumPy array of shape (17, 3) with normalized [y, x, confidence].\n",
    "                    Returns a zero array if no valid person with keypoints is detected.\n",
    "    \"\"\"\n",
    "    # --- THIS IS THE ROBUST FIX ---\n",
    "    # Check if the necessary attributes exist and contain data.\n",
    "    # It's possible to detect boxes but no keypoints, or vice-versa.\n",
    "    if result.keypoints is None or result.boxes is None or len(result.keypoints.data) == 0 or len(result.boxes.data) == 0:\n",
    "        return np.zeros((17, 3), dtype=np.float32)\n",
    "\n",
    "    # Ensure the number of detected boxes matches the number of detected keypoint sets.\n",
    "    # This is a sanity check for potential inconsistencies in the model's output.\n",
    "    if len(result.boxes.data) != len(result.keypoints.data):\n",
    "        # print(\"Warning: Mismatch between number of boxes and keypoints. Skipping frame.\")\n",
    "        return np.zeros((17, 3), dtype=np.float32)\n",
    "\n",
    "    kpts_tensor = result.keypoints.data\n",
    "    boxes_tensor = result.boxes.data\n",
    "\n",
    "    # --- Strategy: Select the person with the largest bounding box ---\n",
    "    # This logic is now safe because we've confirmed boxes_tensor is not empty.\n",
    "    if boxes_tensor.shape[0] > 1:\n",
    "        areas = (boxes_tensor[:, 2] - boxes_tensor[:, 0]) * (boxes_tensor[:, 3] - boxes_tensor[:, 1])\n",
    "        main_person_idx = areas.argmax()\n",
    "    else:\n",
    "        main_person_idx = 0\n",
    "\n",
    "    # Select the keypoints of the main person\n",
    "    person_kpts = kpts_tensor[main_person_idx]\n",
    "    \n",
    "    # Convert to numpy\n",
    "    person_kpts_np = person_kpts.cpu().numpy()\n",
    "    \n",
    "    # Format to [y, x, confidence] and normalize\n",
    "    formatted_kps = np.zeros((17, 3), dtype=np.float32)\n",
    "    for i in range(17):\n",
    "        x, y, conf = person_kpts_np[i]\n",
    "        \n",
    "        # Check for valid coordinates before normalization\n",
    "        if frame_h > 0 and frame_w > 0:\n",
    "            formatted_kps[i] = [y / frame_h, x / frame_w, conf]\n",
    "        else:\n",
    "            formatted_kps[i] = [0.0, 0.0, conf]\n",
    "            \n",
    "    return formatted_kps\n",
    "\n",
    "\n",
    "def extract_keypoints_from_video(video_path, yolo_model):\n",
    "    \"\"\"\n",
    "    Extracts keypoints from all frames of a single video using YOLO.\n",
    "\n",
    "    Args:\n",
    "        video_path (str): Path to the video file.\n",
    "        yolo_model: The loaded YOLO model object.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A NumPy array of shape (num_frames, 17, 3) containing\n",
    "                    normalized keypoints [y, x, confidence] for the main person in the video.\n",
    "    \"\"\"\n",
    "    video_keypoints = []\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Warning: Could not open video {video_path}. Skipping.\")\n",
    "        return None\n",
    "    \n",
    "    num_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    frame_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    \n",
    "    for _ in range(num_frames):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "            \n",
    "        # Run YOLO inference\n",
    "        results = yolo_model(frame, verbose=False)\n",
    "        result = results[0] # Get the first result object\n",
    "\n",
    "        # Process keypoints for the main person\n",
    "        frame_kps = get_main_person_keypoints(result, frame_h, frame_w)\n",
    "        video_keypoints.append(frame_kps)\n",
    "        \n",
    "    cap.release()\n",
    "    \n",
    "    if not video_keypoints:\n",
    "        return None\n",
    "        \n",
    "    return np.array(video_keypoints, dtype=np.float32)\n",
    "\n",
    "\n",
    "def process_all_videos(dataset_dir, output_dir, yolo_model):\n",
    "    \"\"\"\n",
    "    Scans the dataset directory, processes each video, and saves the keypoints.\n",
    "    \"\"\"\n",
    "    # Create the base output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    videos_to_process = []\n",
    "    # Discover all video files and their corresponding output paths\n",
    "    for label_name in [\"Fall\", \"No_Fall\"]:\n",
    "        video_folder = os.path.join(dataset_dir, label_name, \"Raw_Video\")\n",
    "        output_label_folder = os.path.join(output_dir, label_name)\n",
    "        os.makedirs(output_label_folder, exist_ok=True)\n",
    "        \n",
    "        if not os.path.exists(video_folder):\n",
    "            print(f\"Warning: Folder not found {video_folder}\")\n",
    "            continue\n",
    "            \n",
    "        for video_filename in os.listdir(video_folder):\n",
    "            if not video_filename.lower().endswith(('.mp4', '.avi', '.mov')):\n",
    "                continue\n",
    "            \n",
    "            video_path = os.path.join(video_folder, video_filename)\n",
    "            video_basename = os.path.splitext(video_filename)[0]\n",
    "            output_npy_path = os.path.join(output_label_folder, f\"{video_basename}.npy\")\n",
    "            \n",
    "            videos_to_process.append((video_path, output_npy_path))\n",
    "            \n",
    "    print(f\"Found {len(videos_to_process)} videos to process.\")\n",
    "    \n",
    "    # Process each video with a progress bar\n",
    "    pbar = tqdm(videos_to_process, desc=\"Processing videos\")\n",
    "    for video_path, output_npy_path in pbar:\n",
    "        pbar.set_description(f\"Processing {os.path.basename(video_path)}\")\n",
    "        \n",
    "        # This makes the script resumable. If a file exists, skip it.\n",
    "        if os.path.exists(output_npy_path):\n",
    "            continue\n",
    "            \n",
    "        # Extract keypoints\n",
    "        all_keypoints = extract_keypoints_from_video(video_path, yolo_model)\n",
    "        \n",
    "        # Save the result as a .npy file\n",
    "        if all_keypoints is not None:\n",
    "            np.save(output_npy_path, all_keypoints)\n",
    "            \n",
    "    print(\"\\n--- Pre-processing complete! ---\")\n",
    "    print(f\"All keypoint data has been saved to: {output_dir}\")\n",
    "\n",
    "# --- 3. Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    yolo_model = load_yolo_model()\n",
    "    \n",
    "    if yolo_model:\n",
    "        process_all_videos(DATASET_DIR, OUTPUT_DIR, yolo_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
